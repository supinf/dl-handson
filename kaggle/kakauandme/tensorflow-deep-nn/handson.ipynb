{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cc0fdcf1-e813-49d6-95de-16d380347328",
    "_uuid": "66207c9e63c37baa117430625050003f778d063c"
   },
   "source": [
    "# TensorFlow ディープニューラルネットワーク\n",
    "\n",
    "#### MNIST と TensorFlow を使ったディープラーニングチュートリアル\n",
    "by [@kakauandme](https://twitter.com/KaKaUandME) and [@thekoshkina](https://twitter.com/thekoshkina)\n",
    "\n",
    "精度: 0.99\n",
    "\n",
    "**前提条件:** 基礎的なコーディング能力、線形代数（特に行列計算）。画像がコンピュータのメモリ上、どのように保存されているかを知っているとよりよく理解できるしょう。機械学習を始めるには、Andrew Ng 氏による次のサイトをオススメします。[coursera course](https://www.coursera.org/learn/machine-learning)\n",
    "\n",
    "\n",
    "備考:\n",
    "\n",
    "*自由に[このカーネル](https://www.kaggle.com/kakauandme/tensorflow-deep-nn)をフォークして CONSTANTS を調整し、ネットワークの動きがどう変わるか、アルゴリズムのパフォーマンスや精度がどれくらい変わるかをチェックしてみてください。さらに **TensorFlow グラフ** 節もまた、学習のためには変更してみるとよいでしょう。*\n",
    "\n",
    "*100% 理解できないようであれば、変数を都度出力することをオススメします。また、ローカル環境であれば [tensorboard](https://www.tensorflow.org/versions/master/how_tos/summaries_and_tensorboard/index.html) を可視化やデバッグのために利用することもできます。*\n",
    "\n",
    "## ライブラリの導入と設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0a9fc04f-2e04-4a5e-b4e5-cd59a62eb7fa",
    "_uuid": "d81d2735110925d36cf95a02bc06d75741934e5c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# 設定\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# ローカル環境で 0.99 の精度を得たければ、20000 をセットしましょう\n",
    "TRAINING_ITERATIONS = 2500        \n",
    "    \n",
    "DROPOUT = 0.5\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "# 全てを学習データとしてのみ扱うなら 0 をセットします\n",
    "VALIDATION_SIZE = 2000\n",
    "\n",
    "# 試しに出力してみる画像のインデックス\n",
    "IMAGE_TO_DISPLAY = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6261de0c-7f34-4652-9e47-673f0f7e3540",
    "_uuid": "ff59a57d9daa86f4ab1dd7951e4f80bdece93e4e"
   },
   "source": [
    "## データの準備\n",
    "機械学習を始める前に、まずデータを読んでみましょう。*train.csv* は 42,000 行、785 カラムの CSV ファイルです。各行は「手書き数字」と「実際のその値」を表現しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e8225c22-98fe-4077-b9f6-d69f92866dcb",
    "_uuid": "f8eb09b82b2fc13db1b8fa214d93f19db0dd00a4"
   },
   "outputs": [],
   "source": [
    "# CSV ファイルから学習データを読み込みます\n",
    "data = pd.read_csv('./train.csv')\n",
    "\n",
    "print('data({0[0]}, {0[1]})'.format(data.shape))\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5223d3e8-9ac9-409a-8515-1bbbac7902f9",
    "_uuid": "1c8faa2735b5ab8db278a8a8c241b3a23f7a1b79"
   },
   "source": [
    "各画像は、ピクセルの配列に \"引き伸ばされた\" 状態です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a907fe2c-2835-47d2-8def-71c0330d6203",
    "_uuid": "40f9ea9743b0bc24cc61163c068ccdb9f5486496"
   },
   "outputs": [],
   "source": [
    "images = data.iloc[:,1:].values\n",
    "images = images.astype(np.float)\n",
    "\n",
    "# 後続処理のために値を変換します [0:255] => [0.0:1.0]\n",
    "images = np.multiply(images, 1.0 / 255.0)\n",
    "\n",
    "print('images({0[0]}, {0[1]})'.format(images.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "230f0757-29e9-43d8-b0df-a9ac9774067d",
    "_uuid": "8017f5468f76ac8706b6992514d8e2813285fd1b"
   },
   "source": [
    "今回は 784 ピクセルで正方形、つまり 28 * 28px の画像です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5429c6b2-1fa1-4a57-965e-a80c93c07a05",
    "_uuid": "4d1304e97fe0c63f7f1b0abf7e5702178d6383f3"
   },
   "outputs": [],
   "source": [
    "image_size = images.shape[1]\n",
    "print('image_size => {0}'.format(image_size))\n",
    "\n",
    "# 今回すべての画像は正方形です\n",
    "image_width = image_height = np.ceil(np.sqrt(image_size)).astype(np.uint8)\n",
    "\n",
    "print('image_width => {0}\\nimage_height => {1}'.format(image_width,image_height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "32c19f7d-5325-4098-87b4-f1121fe2370f",
    "_uuid": "6f611e6ad4f4ae20728b608c8eb229765bede169"
   },
   "source": [
    "画像を出力するには、この長いピクセル文字列を基本的に白黒画像である 2次元配列に加工します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d99ff9bd-8157-47a9-a953-4c47dbd18500",
    "_uuid": "46c7a4b9742a475d825d80af96af88a0d4bea18f"
   },
   "outputs": [],
   "source": [
    "# 画像の表示\n",
    "def display(img):\n",
    "    \n",
    "    # (784) => (28, 28)\n",
    "    one_image = img.reshape(image_width, image_height)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.imshow(one_image, cmap=cm.binary)\n",
    "\n",
    "# 出力    \n",
    "display(images[IMAGE_TO_DISPLAY])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "27373153-65a5-4524-93a5-6432ebe42543",
    "_uuid": "aee78f64cd83cb2d2abdb0702d522c80bbf570d4"
   },
   "source": [
    "結果として取りうるラベルは 0 から 9 までの数字であり、それは画像そのものが指す数字となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "aa156d75-e95f-49cf-8043-0c098e0b9cda",
    "_uuid": "f288417b32ddddedb214df904e91486e5e3a929f"
   },
   "outputs": [],
   "source": [
    "labels_flat = data.iloc[:,0].values\n",
    "\n",
    "print('labels_flat({0})'.format(len(labels_flat)))\n",
    "print ('labels_flat[{0}] => {1}'.format(IMAGE_TO_DISPLAY, labels_flat[IMAGE_TO_DISPLAY]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a9e99f6e-6610-479f-8d3b-a974b598bd28",
    "_uuid": "f2f64e3e7c6282e453d3a2c890075b937244d4e7"
   },
   "source": [
    "この学習データには 10 種類の 数字 / ラベル / クラス があることを確認しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4e679a33-fa4b-4b81-95ea-ed9516cde16f",
    "_uuid": "6180c79c9142f6dc5262c5313e2bbfd3a458270e"
   },
   "outputs": [],
   "source": [
    "labels_count = np.unique(labels_flat).shape[0]\n",
    "\n",
    "print('labels_count => {0}'.format(labels_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "09e6cdfb-fcdc-4236-9bd9-59c2d31a86a7",
    "_uuid": "6df80b5bccc0031a77c18087058462954f6744e4"
   },
   "source": [
    "ほとんどのクラス分類問題では \"One-hot\" ベクトルが使われます。\"One-hot\" ベクトルとは、ひとつの要素のみが 1 で、そのほかの要素がすべて 0 であるベクトルです。今回、数字 *n* は *n 番目* が 1 である配列、として表現することにしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e7fa1ea8-b67a-4436-b70f-ba75fdeee14a",
    "_uuid": "194046438c98f97c371c419c25c8d2f3cb67f9ea"
   },
   "outputs": [],
   "source": [
    "# クラスのラベルをスカラ値から one-hot ベクトルへ変換\n",
    "# 0 => [1 0 0 0 0 0 0 0 0 0]\n",
    "# 1 => [0 1 0 0 0 0 0 0 0 0]\n",
    "# ...\n",
    "# 9 => [0 0 0 0 0 0 0 0 0 1]\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "labels = dense_to_one_hot(labels_flat, labels_count)\n",
    "labels = labels.astype(np.uint8)\n",
    "\n",
    "print('labels({0[0]}, {0[1]})'.format(labels.shape))\n",
    "print('labels[{0}] => {1}'.format(IMAGE_TO_DISPLAY,labels[IMAGE_TO_DISPLAY]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8df4b1e2-9652-4a40-aa1d-85e4b215d68c",
    "_uuid": "dd520a975b55e3be2d00b9ced50bb649177491a2"
   },
   "source": [
    "最後に、入力データを一部検証用に分割しておきましょう。これは機械学習では必須です。学習には用いられないデータセットを別途用意することで、学習したものが実際うまく一般化されたかを確認することに使えます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b6a6c70b-e172-4673-a04e-48b0e477c3d8",
    "_uuid": "d38ebfb431afd72e0acc999fb17fcd1c1bd16f1c"
   },
   "outputs": [],
   "source": [
    "# 学習 & 検証用に、データを分割する\n",
    "validation_images = images[:VALIDATION_SIZE]\n",
    "validation_labels = labels[:VALIDATION_SIZE]\n",
    "\n",
    "train_images = images[VALIDATION_SIZE:]\n",
    "train_labels = labels[VALIDATION_SIZE:]\n",
    "\n",
    "print('train_images({0[0]}, {0[1]})'.format(train_images.shape))\n",
    "print('validation_images({0[0]}, {0[1]})'.format(validation_images.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "568f1663-e2d7-4f0a-9785-43a993333a6d",
    "_uuid": "38f7dd5dfdd71e45a4d536e7a3329bbda94a21bc"
   },
   "source": [
    "*データの準備はできました。次はニューラルネットワークの構造を定義していきましょう。*\n",
    "\n",
    "## TensorFlow グラフ\n",
    "\n",
    "TensorFlow は Python でないところで重い処理を実行します。そのためすべての操作を独立して実行する代わりに、対話的な操作をひとつの「グラフ」として利用者に定義させ、すべての処理は分離された別プロセスで実行される仕組みとなっています。\n",
    "\n",
    "#### ヘルパー関数\n",
    "\n",
    "ニューラルネットワークモデルのために、たくさんの Weight（重み付け）と Bias が生成されることになります。一般的にいって、Weight は \"対称性の破れ\" を起こし \"ゼロ勾配\" とならないよう、微量のノイズと共に初期化されるべきです。\n",
    "\n",
    " [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) ニューロン（ *f(x)=max(0,x)* という関数を含むもの）を使うので、\"ニューロンの死\" を避けるために少しだけ正値のバイアスを初期化に用いいることもよい方法です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b0ce4f8e-e534-48d4-b69c-978cb38d9cf3",
    "_uuid": "df94996fe1c06d03e04460138f1b3e39baa06494",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Weight 初期化\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dea6c4c6-55a7-4c98-8f8c-b7bd413e6562",
    "_uuid": "2cc40d4f7f99e5298ada4bc785191fbe0a33006d"
   },
   "source": [
    "今回はゼロ埋めした [畳み込み](https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional_layer) を使い、出力サイズを入力サイズと一致させます。この例ではストライド（CNN 用語）をステップで割った値は 1 となります。\n",
    "\n",
    "一般に、畳み込みレイヤはデータの特徴を抽出することに使われます。形から数字を読む数字認識においていえば、それぞれ特定の形に反応するカーネルやフィルタ（いずれも CNN 用語）を使うことになるでしょう。フィルタの数は解きたい問題によって変わり得ます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9a768a1c-f476-4b64-90be-c562a7ebe262",
    "_uuid": "b21cc84c4bd9a47e38d756cb10c026fdb354b7c6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 畳み込み\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5307a605-de69-45a2-a0f6-6d9b55de97ec",
    "_uuid": "301a72061f15e1d95b1df46eca4c8096b7061a03"
   },
   "source": [
    "[プーリング](https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer) は 2x2 ブロックの純粋な Max Pooling 関数です。\n",
    "\n",
    "プーリングは重要な情報は残しながら、扱うデータを小さくすることに使われます。2x2 の Max Pooling 関数は画像を 2 ピクセル四方のブロックに分割し、そのブロック内での最大値のみを返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c117a3a0-09b1-4a4a-9b54-45d6f6f6ac72",
    "_uuid": "93a6524c5234fcd053904fb2ddc88789725466e6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# プーリング\n",
    "# [[0,3],\n",
    "#  [4,2]] => 4\n",
    "\n",
    "# [[0,1],\n",
    "#  [1,1]] => 1\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a608c7b1-81a8-4a7c-96cd-fd4ccac98e42",
    "_uuid": "f8e9fdd91793d75de3019d30d6ec01f98b617ab6"
   },
   "source": [
    "*畳み込みとプーリングについては後でまた詳細に踏み込みます*\n",
    "\n",
    "どんなニューラルネットワークでもとても深いレイヤに組み込めるというメリットが意味するのは、ひとつのレイヤの結果が、次のレイヤの入力として利用できるということです。この逐次的アプローチを使えば、とても繊細な、とても深いニューラルネットのレイヤが構成できます。これをディープニューラルネットワークと呼びます。\n",
    "\n",
    "今回はプーリング層を間に挟んだ 2 つの畳み込み層に続け、全結合層、ドロップアウト、そして出力層で構成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2076ed52-4be3-4665-8cf4-c3e5f168f9bd",
    "_uuid": "cf49b6bedb371504bdaab9ae51722b298a597a95"
   },
   "outputs": [],
   "source": [
    "# ニューラルネットワークの入力 & 出力\n",
    "\n",
    "# 画像\n",
    "x = tf.placeholder('float', shape=[None, image_size])\n",
    "# ラベル\n",
    "y_ = tf.placeholder('float', shape=[None, labels_count])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2781b615-ecbd-467a-8021-0e4fd1ff75b9",
    "_uuid": "bf8c49b12be2464d6534e7e595b58e2fc75c0492"
   },
   "source": [
    "最初の層は畳み込み、次に Max pooling と続きます。畳み込みでは各 5x5 ピクセルごと、32 の特徴が計算されます。この時 Weight テンソルは [5, 5, 1, 32] という形となります。最初の 2 次元はパッチサイズ、3 つ目は入力チャンネル（最初の層では色味、つまり 1 とは白黒のこと）、最後のものは出力サイズです。バイアス配列もまた、各出力チャンネルに配置されています。\n",
    "\n",
    "このレイヤを適用するために、入力データも 4 次元のテンソルに整形します。最初の次元は画像数、2、3 番目は画像の幅と高さ、4 番目は色チェンネルです。\n",
    "\n",
    "畳み込みのあとは、プーリング層によって出力サイズが 28x28 から 14x14 に縮小されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e781e8a9-6b73-4227-a85e-d4bca343de1f",
    "_uuid": "2bc648fe5f03c79e197b44f203d4045e7dcbbc90",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 つ目の畳み込み層\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# (40000,784) => (40000,28,28,1)\n",
    "image = tf.reshape(x, [-1,image_width , image_height,1])\n",
    "# print(image.get_shape()) # =>(40000,28,28,1)\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(image, W_conv1) + b_conv1)\n",
    "# print(h_conv1.get_shape()) # => (40000, 28, 28, 32)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "# print(h_pool1.get_shape()) # => (40000, 14, 14, 32)\n",
    "\n",
    "# 可視化のための準備\n",
    "# 4x8 の行列として 32 の特徴を表示\n",
    "layer1 = tf.reshape(h_conv1, (-1, image_height, image_width, 4 ,8))  \n",
    "\n",
    "# チャンネルが最初の次元になり、それに x, y が続く様に並び替え\n",
    "layer1 = tf.transpose(layer1, (0, 3, 1, 4,2))\n",
    "layer1 = tf.reshape(layer1, (-1, image_height*4, image_width*8)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e3fe3f3b-a98b-43fa-a890-aa7ac917db73",
    "_uuid": "7cf8b345e9852462717928f862ecc6e45ce2a468"
   },
   "source": [
    "2 番目のレイヤは 5x5 ピクセルごとに、64 の特徴を抽出します。Weight テンソルは [5, 5, 32, 64] という形をしています。最初の 2 次元はパッチサイズ、その次は入力チャンネル数（この前のレイヤで得たのは 32 の特徴でしたね）、そして最後は出力チャンネル数です。バイアス配列もまた、各出力チャンネルに配置されています。\n",
    "\n",
    "画像はプーリングによって 14x14 まで縮小されているため、2 つ目の畳み込み層では、画像のより一般的な特徴をピックアップします。フィルタの適用範囲はより広くなります。それにより、最初の層ではディテールをみていましたが、2 層目ではより総括的な特徴抽出が可能になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e47a667c-391d-4393-838c-691d7cf90aa4",
    "_uuid": "2f5ac558d461ee1bb405dae147114a8a1d46deca",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 つ目の畳み込み層\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "# print(h_conv2.get_shape()) # => (40000, 14,14, 64)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "# print(h_pool2.get_shape()) # => (40000, 7, 7, 64)\n",
    "\n",
    "# 可視化のための準備\n",
    "# 4x16 の行列として 64 の特徴を表示\n",
    "layer2 = tf.reshape(h_conv2, (-1, 14, 14, 4 ,16))  \n",
    "\n",
    "# チャンネルが最初の次元になり、それに x, y が続く様に並び替え\n",
    "layer2 = tf.transpose(layer2, (0, 3, 1, 4,2))\n",
    "layer2 = tf.reshape(layer2, (-1, 14*4, 14*16)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "61ce56de-7686-42ad-83ce-0bb152eb4f84",
    "_uuid": "9f0cfe8917908f8bb7bd7f7b8652773c3f93fadf"
   },
   "source": [
    "画像サイズが 7x7 まで縮小されたので、ここで 1024 個のニューロンからなる [全結合層](https://en.wikipedia.org/wiki/Convolutional_neural_network#Fully_Connected_layer) を使い、いよいよ画像全体での処理結果を求めてみましょう。（全結合層の各ニューロンは、前の層のすべての活性化関数 / 出力と繋がっています）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "00b999e1-50d3-4dcc-b7db-45b8f62d739c",
    "_uuid": "2cfee8680affcf944ff542af11841c222daf96db",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 全結合\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "# (40000, 7, 7, 64) => (40000, 3136)\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "# print(h_fc1.get_shape()) # => (40000, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9684a32e-bf07-43bc-bad2-7fff8a2e0387",
    "_uuid": "6bc2f697ae8f16027fbff676cc6a58010a583b65"
   },
   "source": [
    "過学習を防止するために、出力層の前に[ドロップアウト](https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout) を使います。\n",
    "\n",
    "ドロップアウトは各学習段階のネットワークから、いくつかのノードを除外します。各ノードの値は確率 *keep_prob* に応じて伝播されるか、確率 *1 - keep_prob* に応じて省かれます。学習段階が終わった後は、各ノードは初期の Weight でニューラルネットワークに戻されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "473023e9-cd24-4e7b-a352-e446bf46af74",
    "_uuid": "4971a77e0cbf9f20e14c79c671370af0849781ca",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ドロップアウト\n",
    "keep_prob = tf.placeholder('float')\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "db3645c5-8cce-488f-9a41-6b7767b0e4f7",
    "_uuid": "f4892e3e64c9bd94027598e9fe0fb0f66a2383a7"
   },
   "source": [
    "最後に、Softmax 層を追加します。単純な [Softmax 回帰](https://en.wikipedia.org/wiki/Softmax_function) と同じです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fec80f9e-8e1e-4a9a-b045-6b7adb76fb56",
    "_uuid": "6f3a82d4499cb35b6c31b7dfdcbb6fbd4670b698"
   },
   "outputs": [],
   "source": [
    "# 深層のための出力層\n",
    "W_fc2 = weight_variable([1024, labels_count])\n",
    "b_fc2 = bias_variable([labels_count])\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "# print(y.get_shape()) # => (40000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e86f33ce-9357-43b3-8c57-18ad57dd8c72",
    "_uuid": "7bf623abcf9c21873b7d8e8cdf498db0a893cf46"
   },
   "source": [
    "ネットワークのパフォーマンスを評価するために、[クロスエントロピー](https://en.wikipedia.org/wiki/Cross_entropy) を使い、それを最小化するために [ADAM 最適化](http://arxiv.org/pdf/1412.6980v8.pdf) を使います。\n",
    "\n",
    "ADAM 最適化は勾配ベースの最適化アルゴリズムであり、適応性のある推定に基づいています。急激な勾配降下法よりも洗練されていて、巨大なデータや膨大なパラメタとなる課題に適しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c1c6bd1d-e258-4427-8cf1-6e2855c5b035",
    "_uuid": "c098538c8e621ae4f42d51248079793be53e3fa8"
   },
   "outputs": [],
   "source": [
    "# コスト関数\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "# 最適化関数\n",
    "train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "\n",
    "# 評価\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6c1610c3-3c64-429f-826a-e0d4220b1f92",
    "_uuid": "9bad13229debfa63eed17e6ca80cc8cf69840828"
   },
   "source": [
    "テストデータから推論するには、0 から 9 の数字のうちもっとも高い可能性だと示唆する \"One-hot ベクトル\" から、もっとも高い確率のものが選択されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "693c4f8a-c654-45a6-8d77-aae332397f8f",
    "_uuid": "835cb34320512a14ba659f7814f4adc90cfb5f9d"
   },
   "outputs": [],
   "source": [
    "# 推論関数\n",
    "#[0.1, 0.9, 0.2, 0.1, 0.1 0.3, 0.5, 0.1, 0.2, 0.3] => 1\n",
    "predict = tf.argmax(y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "72bfde42-54ce-4d31-bd67-abee9be48527",
    "_uuid": "1782cb86efec632f983f4b26d2c832535fde62f9"
   },
   "source": [
    "*最後にニューラルネットワークの構造が決まったら、学習に向けての TensorFlow グラフの準備は万端です。*\n",
    "\n",
    "## 学習、検証そして推論\n",
    "\n",
    "#### ヘルパー関数\n",
    "\n",
    "理想的には学習の各ステップですべてのデータを使うべきですが、それでは高くつきます。そこで代わりに、ランダムな小さなデータとして \"バッチ\" を使います。\n",
    "\n",
    "この手法は [確率的学習](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) と言います。より安く、速く、そしてほぼ同じ結果を得ることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8dfbde97-c69a-4ca2-8d09-0b0847e8b1d0",
    "_uuid": "8daaab41e4444102bf346baebeb9cee43725db35"
   },
   "outputs": [],
   "source": [
    "epochs_completed = 0\n",
    "index_in_epoch = 0\n",
    "num_examples = train_images.shape[0]\n",
    "\n",
    "# バッチでのデータ投入\n",
    "def next_batch(batch_size):\n",
    "    \n",
    "    global train_images\n",
    "    global train_labels\n",
    "    global index_in_epoch\n",
    "    global epochs_completed\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    # すべてのデータが学習に使われたらランダムに並び替える    \n",
    "    if index_in_epoch > num_examples:\n",
    "        # 終了エポック\n",
    "        epochs_completed += 1\n",
    "        # データのシャッフル\n",
    "        perm = np.arange(num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        train_images = train_images[perm]\n",
    "        train_labels = train_labels[perm]\n",
    "        # 次のエポック\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "        assert batch_size <= num_examples\n",
    "    end = index_in_epoch\n",
    "    return train_images[start:end], train_labels[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "263be179-83cc-4abb-baa1-12c5126d7693",
    "_uuid": "8919021a04dbc664cc8a9b0ceb74046b8be1eb25"
   },
   "source": [
    "さて、TensorFlow のグラフとして、すべての変数に対してすべての操作が定義できました。すべての計算は Python 外の環境で実行されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0406f6b1-3d14-446b-b7d4-dd32ac1fb98c",
    "_uuid": "00f16eed7e1d97d58a7fa7468c0b41727ee158bb"
   },
   "outputs": [],
   "source": [
    "# TensorFlow セッションの開始\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1607afc9-d617-4635-a667-3b415e544cab",
    "_uuid": "ec4dc5de940f01cc4927884bad4acceae3f1bbc8"
   },
   "source": [
    "ループでの各ステップでは、学習データセットから \"バッチ\" を取得し、それを事前にグラフとして定義したプレースホルダと置換しながら学習を進めます。ここでは *x, y* や *dropout* にあたるものです。\n",
    "\n",
    "また、時折、次回の \"バッチ\" に対して学習精度をチェックします。\n",
    "\n",
    "ローカル環境では、[学習プロセスの保存](https://www.tensorflow.org/versions/master/api_docs/python/state_ops.html#Saver) をオススメします。これにより将来の学習やデバック、評価のためにデータをリカバリすることができるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "076a4e38-50ca-48b1-b9ca-cfd53cb2b31c",
    "_uuid": "972ae79a21ae0b68da9d9e2a917b75c50273527c"
   },
   "outputs": [],
   "source": [
    "# 変数の可視化\n",
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "x_range = []\n",
    "\n",
    "display_step=1\n",
    "\n",
    "for i in range(TRAINING_ITERATIONS):\n",
    "\n",
    "    # 新しいバッチの取得\n",
    "    batch_xs, batch_ys = next_batch(BATCH_SIZE)        \n",
    "\n",
    "    # 各 1, 2, ..., 10, 20, ..., 100... ステップでチェックする\n",
    "    if i%display_step == 0 or (i+1) == TRAINING_ITERATIONS:\n",
    "        \n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_xs, \n",
    "                                                  y_: batch_ys, \n",
    "                                                  keep_prob: 1.0})       \n",
    "        if(VALIDATION_SIZE):\n",
    "            validation_accuracy = accuracy.eval(feed_dict={ x: validation_images[0:BATCH_SIZE], \n",
    "                                                            y_: validation_labels[0:BATCH_SIZE], \n",
    "                                                            keep_prob: 1.0})                                  \n",
    "            print('training_accuracy / validation_accuracy => %.2f / %.2f for step %d'%(train_accuracy, validation_accuracy, i))\n",
    "            \n",
    "            validation_accuracies.append(validation_accuracy)\n",
    "            \n",
    "        else:\n",
    "             print('training_accuracy => %.4f for step %d'%(train_accuracy, i))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        x_range.append(i)\n",
    "        \n",
    "        # 表示用ステップを更新する\n",
    "        if i%(display_step*10) == 0 and i:\n",
    "            display_step *= 10\n",
    "\n",
    "    # バッチを学習する\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: DROPOUT})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9fd5c9d6-2a52-4e9b-b3a9-7561d94276ce",
    "_uuid": "f3b586b38e693002c89bf91a7b6e28f3bf415d9d"
   },
   "source": [
    "学習が終わったら、学習に使われなかったデータで精度を確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "35af75e6-0d24-43e4-86bc-ddffc18adce0",
    "_uuid": "e8340a91a24388032a6f8b85a19fa8acb32e3887"
   },
   "outputs": [],
   "source": [
    "# 評価用セットで最終的な精度を確認する\n",
    "if(VALIDATION_SIZE):\n",
    "    validation_accuracy = accuracy.eval(feed_dict={x: validation_images, \n",
    "                                                   y_: validation_labels, \n",
    "                                                   keep_prob: 1.0})\n",
    "    print('validation_accuracy => %.4f'%validation_accuracy)\n",
    "    plt.plot(x_range, train_accuracies,'-b', label='Training')\n",
    "    plt.plot(x_range, validation_accuracies,'-g', label='Validation')\n",
    "    plt.legend(loc='lower right', frameon=False)\n",
    "    plt.ylim(ymax = 1.1, ymin = 0.7)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('step')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e9c2e94a-b34e-455d-b1e6-74ad834c25fa",
    "_uuid": "80350e800843ac59616ffeadb022edf73cf86942"
   },
   "source": [
    "結果が得られるのはうれしいことですが、*test.csv* からテストデータを読み込み、その画像群に対してもラベルを推論してみましょう。\n",
    "\n",
    "テストデータは画像データのみを含んでおり、ラベル情報がありません。それ以外は、その構造は学習データと似ています。\n",
    "\n",
    "推論されたラベルは CSV に保存されます。いずれ競技会へ Submit してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ae18cc64-ddcc-4d5c-abc5-d5e36ffc04e8",
    "_uuid": "79832bf51c06c719bb6fe6ae49dd5476f1ceb669"
   },
   "outputs": [],
   "source": [
    "# CSV ファイルからテストデータを読み込む\n",
    "test_images = pd.read_csv('./test.csv').values\n",
    "test_images = test_images.astype(np.float)\n",
    "\n",
    "# 値を変換する [0:255] => [0.0:1.0]\n",
    "test_images = np.multiply(test_images, 1.0 / 255.0)\n",
    "print('test_images({0[0]}, {0[1]})'.format(test_images.shape))\n",
    "\n",
    "# テストデータでの推論\n",
    "#predicted_lables = predict.eval(feed_dict={x: test_images, keep_prob: 1.0})\n",
    "\n",
    "# リソースをより効率的に使うためにバッチで計算する\n",
    "predicted_lables = np.zeros(test_images.shape[0])\n",
    "for i in range(0,test_images.shape[0]//BATCH_SIZE):\n",
    "    predicted_lables[i*BATCH_SIZE : (i+1)*BATCH_SIZE] = predict.eval(feed_dict={x: test_images[i*BATCH_SIZE : (i+1)*BATCH_SIZE], \n",
    "                                                                                keep_prob: 1.0})\n",
    "print('predicted_lables({0})'.format(len(predicted_lables)))\n",
    "\n",
    "# テスト画像とその推論結果を出力する\n",
    "display(test_images[IMAGE_TO_DISPLAY])\n",
    "print('predicted_lables[{0}] => {1}'.format(IMAGE_TO_DISPLAY,predicted_lables[IMAGE_TO_DISPLAY]))\n",
    "\n",
    "# 結果を保存する\n",
    "np.savetxt('submission_softmax.csv', \n",
    "           np.c_[range(1,len(test_images)+1),predicted_lables], \n",
    "           delimiter=',', \n",
    "           header = 'ImageId,Label', \n",
    "           comments = '', \n",
    "           fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b4f0d60f-8c0f-4769-b555-f47de4c3e60f",
    "_uuid": "b62b8b410d6667a01710013b439c29c47c9b7f47"
   },
   "source": [
    "## 付録\n",
    "\n",
    "以前示唆したように、処理をよりよく理解するためには変数を出力してみるのがよいでしょう。\n",
    "\n",
    "以下は TensorFlow グラフから、最初の畳み込み層を出力するものです。32 の特徴を格子状に画像へ変換しますが、異なる画像からニューラルネットワークによって大まかな特徴がフィルタリングされる様子はとても興味深いかと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0c65a916-040c-4d55-afbc-438954ce1695",
    "_uuid": "03a2d1d3f20e6fea756d6035fed506a6effdc073"
   },
   "outputs": [],
   "source": [
    "layer1_grid = layer1.eval(feed_dict={x: test_images[IMAGE_TO_DISPLAY:IMAGE_TO_DISPLAY+1], keep_prob: 1.0})\n",
    "plt.axis('off')\n",
    "plt.imshow(layer1_grid[0], cmap=cm.seismic )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f0830c2b-1c95-4c8b-b3d0-45f761e8da46",
    "_uuid": "84139897f4facbc4725650b486cac236033e4432",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d547c90e-30f4-45ef-9aae-3cfe8bbfebe4",
    "_uuid": "9e1acc14703038e610773004eabb2c4b7d9700fe"
   },
   "source": [
    "## 参考情報\n",
    "\n",
    "- [Deep MNIST for Experts](https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html#deep-mnist-for-experts)\n",
    "- [A Convolutional Network implementation example using TensorFlow library](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3%20-%20Neural%20Networks/convolutional_network.ipynb)\n",
    "- [Digit recognizer in Python using CNN](https://www.kaggle.com/kobakhit/digit-recognizer/digit-recognizer-in-python-using-cnn)\n",
    "- [Deep Learning in a Nutshell: Core Concepts](http://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
